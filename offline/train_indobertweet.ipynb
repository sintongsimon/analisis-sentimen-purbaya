!pip install transformers datasets torch scikit-learn

import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

class TweetDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=128
        )
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="weighted"
    )
    acc = accuracy_score(labels, preds)

    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

def tokenize(texts):
    return tokenizer(
        list(texts),
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )


INPUT_DIR  = "/content/drive/MyDrive/Colab Notebooks/Script"
filepath = f"{INPUT_DIR}/preprocessed_tweets_merged_2025-09-08_to_2025-12-31.csv"

df = pd.read_csv(filepath)

df = df[['clean_text_bert', 'label']]
df = df.dropna()

df['label'].value_counts()

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "indolem/indobertweet-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

metrics_all = []

for fold, (train_idx, val_idx) in enumerate(skf.split(df.clean_text_bert, df.label), 1):
    print(f"\n===== FOLD {fold} =====")

    train_dataset = TweetDataset(
        df.clean_text_bert.iloc[train_idx].tolist(),
        df.label.iloc[train_idx].tolist()
    )
    val_dataset = TweetDataset(
        df.clean_text_bert.iloc[val_idx].tolist(),
        df.label.iloc[val_idx].tolist()
    )

    model = BertForSequenceClassification.from_pretrained(
        "indolem/indobertweet-base-uncased",
        num_labels=2
    )

    training_args = TrainingArguments(
        output_dir=f"./results/fold_{fold}",
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        logging_dir="./logs",
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    trainer.train()
    metrics = trainer.evaluate()

    metrics["fold"] = fold
    metrics_all.append(metrics)

    # simpan model (ambil fold terakhir atau terbaik)
    model.save_pretrained(f"{INPUT_DIR}/model/indobertweet_sentiment/{fold}")
    tokenizer.save_pretrained(f"{INPUT_DIR}/model/indobertweet_sentiment/{fold}")

metrics_df = pd.DataFrame(metrics_all)
metrics_df.to_csv(f"{INPUT_DIR}/metrics_indobertweet.csv", index=False)

print(f"\n===== Done =====")

